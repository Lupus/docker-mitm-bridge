apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: {{ .Release.Name }}-inject-proxy
spec:
  background: false
  validationFailureAction: Audit
  rules:
  # Inject init container
  - name: inject-init-container
    match:
      any:
      - resources:
          kinds:
          - Pod
          selector:
            matchLabels:
              "{{ .Values.kyverno.annotationKey }}": "{{ .Values.kyverno.annotationValue }}"
    mutate:
      patchStrategicMerge:
        spec:
          initContainers:
          - name: proxy-init
            image: {{ .Values.initContainer.image }}
            imagePullPolicy: IfNotPresent
            securityContext:
              capabilities:
                add:
                - NET_ADMIN
                - NET_RAW
              runAsNonRoot: false
              runAsUser: 0
            command:
            - /bin/sh
            - -c
            - |
              set -ex

              # Generate or load CA certificate
              echo "Setting up CA certificate..."

              # Check if CA already exists in the shared volume
              if [ -f /ca-secret/tls.crt ] && [ -f /ca-secret/tls.key ]; then
                echo "CA already exists, using existing CA"
              else
                echo "Generating new CA certificate..."

                # Generate CA private key
                openssl genrsa -out /ca-secret/tls.key 2048

                # Generate CA certificate (self-signed, valid for 10 years)
                openssl req -new -x509 -key /ca-secret/tls.key -out /ca-secret/tls.crt -days 3650 \
                  -subj "/C=US/ST=CA/O=Internal-CA/CN=Kyverno-Intercept-CA"

                echo "CA certificate generated successfully"
              fi

              # Set permissions so xDS service (UID {{ .Values.xds.uid }}) can read the CA files
              # CA certificate is public - readable by all
              chmod 644 /ca-secret/tls.crt
              # CA private key MUST be restricted - only xDS service should read it
              chmod 600 /ca-secret/tls.key
              chown {{ .Values.xds.uid }}:{{ .Values.xds.gid }} /ca-secret/tls.key

              # Copy CA cert to shared volume for application containers
              cp /ca-secret/tls.crt /shared-ca-certs/intercept-ca.crt

              # Merge with system certificates for containers that need it
              cat /etc/ssl/certs/ca-certificates.crt /ca-secret/tls.crt > /shared-ca-certs/ca-certificates.crt 2>/dev/null || cp /ca-secret/tls.crt /shared-ca-certs/ca-certificates.crt

              echo "CA certificate installed successfully"

              # Setup iptables rules for traffic interception
              echo "Setting up iptables rules..."

              ENVOY_UID={{ .Values.envoy.uid }}
              OPA_UID={{ .Values.opa.uid }}
              {{- if .Values.xds.enabled }}
              XDS_UID={{ .Values.xds.uid }}
              {{- end }}
              ENVOY_HTTP_PORT={{ .Values.envoy.port }}
              ENVOY_HTTPS_PORT={{ .Values.envoy.httpsPort }}

              # ============================================
              # NAT table - Traffic redirection
              # ============================================
              iptables -t nat -N PROXY_REDIRECT 2>/dev/null || true
              iptables -t nat -F PROXY_REDIRECT

              # CRITICAL: Exclude sidecar traffic to prevent infinite loops
              iptables -t nat -A PROXY_REDIRECT -m owner --uid-owner ${ENVOY_UID} -j RETURN
              iptables -t nat -A PROXY_REDIRECT -m owner --uid-owner ${OPA_UID} -j RETURN
              {{- if .Values.xds.enabled }}
              iptables -t nat -A PROXY_REDIRECT -m owner --uid-owner ${XDS_UID} -j RETURN
              {{- end }}

              # Redirect HTTP/HTTPS traffic to separate Envoy listeners
              iptables -t nat -A PROXY_REDIRECT -p tcp --dport 80 -j REDIRECT --to-port ${ENVOY_HTTP_PORT}
              iptables -t nat -A PROXY_REDIRECT -p tcp --dport 443 -j REDIRECT --to-port ${ENVOY_HTTPS_PORT}

              # Apply redirection
              iptables -t nat -A OUTPUT -p tcp -j PROXY_REDIRECT

              # ============================================
              # FILTER table - Port isolation
              # ============================================
              iptables -t filter -N SIDECAR_ISOLATE 2>/dev/null || true
              iptables -t filter -F SIDECAR_ISOLATE

              # Allow all traffic from Envoy/OPA/SDS sidecars
              iptables -t filter -A SIDECAR_ISOLATE -m owner --uid-owner ${ENVOY_UID} -j ACCEPT
              iptables -t filter -A SIDECAR_ISOLATE -m owner --uid-owner ${OPA_UID} -j ACCEPT
              {{- if .Values.xds.enabled }}
              iptables -t filter -A SIDECAR_ISOLATE -m owner --uid-owner ${XDS_UID} -j ACCEPT
              {{- end }}

              # For main container accessing localhost:

              # 1. Block privileged ports (0-{{ .Values.initContainer.privilegedPortEnd }}) for safety
              iptables -t filter -A SIDECAR_ISOLATE -d 127.0.0.1 -p tcp --dport 0:{{ .Values.initContainer.privilegedPortEnd }} -j REJECT --reject-with tcp-reset

              # 2. Allow redirected traffic to Envoy proxy ports (needed after NAT redirect)
              iptables -t filter -A SIDECAR_ISOLATE -d 127.0.0.1 -p tcp --dport {{ .Values.envoy.port }} -j ACCEPT
              iptables -t filter -A SIDECAR_ISOLATE -d 127.0.0.1 -p tcp --dport {{ .Values.envoy.httpsPort }} -j ACCEPT

              # 3. Block other sidecar infrastructure ports ({{ .Values.initContainer.sidecarPortRangeStart }}-{{ .Values.initContainer.sidecarPortRangeEnd }})
              iptables -t filter -A SIDECAR_ISOLATE -d 127.0.0.1 -p tcp --dport {{ .Values.initContainer.sidecarPortRangeStart }}:{{ .Values.initContainer.sidecarPortRangeEnd }} -j REJECT --reject-with tcp-reset

              # 4. Allow everything else on localhost (app development ports)
              iptables -t filter -A SIDECAR_ISOLATE -d 127.0.0.1 -j ACCEPT

              # Allow DNS (required for hostname resolution)
              iptables -t filter -A SIDECAR_ISOLATE -p udp --dport 53 -j ACCEPT
              iptables -t filter -A SIDECAR_ISOLATE -p tcp --dport 53 -j ACCEPT

              # Allow outbound HTTP/HTTPS to internet (will be redirected by NAT)
              {{- range .Values.initContainer.redirectPorts }}
              iptables -t filter -A SIDECAR_ISOLATE -p tcp --dport {{ . }} -j ACCEPT
              {{- end }}

              # Block all other outbound traffic from main container
              # Using DROP instead of REJECT for catch-all rule to avoid iptables compatibility issues
              iptables -t filter -A SIDECAR_ISOLATE -j DROP

              # Apply isolation
              iptables -t filter -A OUTPUT -j SIDECAR_ISOLATE

              # IPv6 filtering (best-effort, silently fails if IPv6 unavailable)
              {
                ip6tables -t nat -N PROXY_REDIRECT 2>/dev/null
                ip6tables -t nat -F PROXY_REDIRECT
                ip6tables -t nat -A PROXY_REDIRECT -m owner --uid-owner ${ENVOY_UID} -j RETURN
                ip6tables -t nat -A PROXY_REDIRECT -m owner --uid-owner ${OPA_UID} -j RETURN
                {{- if .Values.xds.enabled }}
                ip6tables -t nat -A PROXY_REDIRECT -m owner --uid-owner ${XDS_UID} -j RETURN
                {{- end }}
                ip6tables -t nat -A PROXY_REDIRECT -p tcp --dport 80 -j REDIRECT --to-port ${ENVOY_HTTP_PORT}
                ip6tables -t nat -A PROXY_REDIRECT -p tcp --dport 443 -j REDIRECT --to-port ${ENVOY_HTTPS_PORT}
                ip6tables -t nat -A OUTPUT -p tcp -j PROXY_REDIRECT
                ip6tables -t filter -N SIDECAR_ISOLATE 2>/dev/null
                ip6tables -t filter -F SIDECAR_ISOLATE
                ip6tables -t filter -A SIDECAR_ISOLATE -m owner --uid-owner ${ENVOY_UID} -j ACCEPT
                ip6tables -t filter -A SIDECAR_ISOLATE -m owner --uid-owner ${OPA_UID} -j ACCEPT
                {{- if .Values.xds.enabled }}
                ip6tables -t filter -A SIDECAR_ISOLATE -m owner --uid-owner ${XDS_UID} -j ACCEPT
                {{- end }}
                ip6tables -t filter -A SIDECAR_ISOLATE -d ::1 -p tcp --dport 0:{{ .Values.initContainer.privilegedPortEnd }} -j REJECT --reject-with tcp-reset
                ip6tables -t filter -A SIDECAR_ISOLATE -d ::1 -p tcp --dport {{ .Values.envoy.port }} -j ACCEPT
                ip6tables -t filter -A SIDECAR_ISOLATE -d ::1 -p tcp --dport {{ .Values.envoy.httpsPort }} -j ACCEPT
                ip6tables -t filter -A SIDECAR_ISOLATE -d ::1 -p tcp --dport {{ .Values.initContainer.sidecarPortRangeStart }}:{{ .Values.initContainer.sidecarPortRangeEnd }} -j REJECT --reject-with tcp-reset
                ip6tables -t filter -A SIDECAR_ISOLATE -d ::1 -j ACCEPT
                ip6tables -t filter -A SIDECAR_ISOLATE -p udp --dport 53 -j ACCEPT
                ip6tables -t filter -A SIDECAR_ISOLATE -p tcp --dport 53 -j ACCEPT
                {{- range .Values.initContainer.redirectPorts }}
                ip6tables -t filter -A SIDECAR_ISOLATE -p tcp --dport {{ . }} -j ACCEPT
                {{- end }}
                ip6tables -t filter -A SIDECAR_ISOLATE -j DROP
                ip6tables -t filter -A OUTPUT -j SIDECAR_ISOLATE
              } >/dev/null 2>&1 || true

              # List rules for debugging
              echo "iptables rules applied:"
              echo "=== NAT table ==="
              iptables -t nat -L -v -n
              echo "=== FILTER table ==="
              iptables -t filter -L -v -n

              echo "Init container completed successfully!"
              echo "- Main container can make HTTP/HTTPS requests (redirected to Envoy)"
              echo "- Main container can use localhost for development"
              echo "- Main container CANNOT access privileged ports (0-{{ .Values.initContainer.privilegedPortEnd }})"
              echo "- Main container CANNOT access sidecar ports ({{ .Values.initContainer.sidecarPortRangeStart }}-{{ .Values.initContainer.sidecarPortRangeEnd }})"
            volumeMounts:
            - name: ca-secret
              mountPath: /ca-secret
              readOnly: false  # Init container needs write access to generate CA
            - name: ca-certificates
              mountPath: /shared-ca-certs

  {{- if .Values.opa.enabled }}
  # Inject OPA data setup init container as a separate rule
  - name: inject-opa-init-container
    match:
      any:
      - resources:
          kinds:
          - Pod
          selector:
            matchLabels:
              "{{ .Values.kyverno.annotationKey }}": "{{ .Values.kyverno.annotationValue }}"
    mutate:
      patchStrategicMerge:
        spec:
          initContainers:
          - name: opa-data-setup
            image: {{ .Values.initContainer.image }}  # Use same image as proxy-init (has shell)
            imagePullPolicy: IfNotPresent
            command:
            - /bin/sh
            - -c
            - |
              set -e
              echo "Setting up OPA policy data..."

              # Check if custom OPA data is provided via annotation (downwardAPI volume)
              if [ -f /podinfo/opa-data ] && [ -s /podinfo/opa-data ]; then
                echo "Using custom OPA policy data from annotation"
                cp /podinfo/opa-data /opa-data/data.yaml
              else
                echo "Using default OPA policy data from ConfigMap"
                cp /policies-default/data.yml /opa-data/data.yaml
              fi

              echo "OPA data file prepared at /opa-data/data.yaml"
              ls -la /opa-data/
              echo "Data file contents:"
              cat /opa-data/data.yaml
            volumeMounts:
            - name: opa-data
              mountPath: /opa-data
            - name: opa-policy
              mountPath: /policies-default
              readOnly: true
            - name: podinfo
              mountPath: /podinfo
              readOnly: true
            securityContext:
              runAsUser: {{ .Values.opa.uid }}
              runAsGroup: {{ .Values.opa.gid }}
              runAsNonRoot: true
              allowPrivilegeEscalation: false
              capabilities:
                drop:
                - ALL
  {{- end }}

  # Inject Envoy sidecar
  - name: inject-envoy-sidecar
    match:
      any:
      - resources:
          kinds:
          - Pod
          selector:
            matchLabels:
              "{{ .Values.kyverno.annotationKey }}": "{{ .Values.kyverno.annotationValue }}"
    mutate:
      patchStrategicMerge:
        spec:
          containers:
          - name: envoy-proxy
            image: {{ .Values.envoy.image }}
            imagePullPolicy: IfNotPresent
            args:
            - "-c"
            - "/etc/envoy/config/envoy.yaml"
            - "--log-level"
            - "{{ .Values.envoy.logLevel }}"
            ports:
            - name: proxy
              containerPort: {{ .Values.envoy.port }}
              protocol: TCP
            - name: admin
              containerPort: {{ .Values.envoy.adminPort }}
              protocol: TCP
            resources:
              {{- toYaml .Values.envoy.resources | nindent 14 }}
            securityContext:
              runAsUser: {{ .Values.envoy.uid }}
              runAsGroup: {{ .Values.envoy.gid }}
              runAsNonRoot: true
              allowPrivilegeEscalation: false
              capabilities:
                drop:
                - ALL
              readOnlyRootFilesystem: false
            volumeMounts:
            - name: envoy-config
              mountPath: /etc/envoy/config
              readOnly: true
            livenessProbe:
              httpGet:
                path: /ready
                port: admin
              initialDelaySeconds: 10
              periodSeconds: 5
            readinessProbe:
              httpGet:
                path: /ready
                port: admin
              initialDelaySeconds: 5
              periodSeconds: 5

          {{- if .Values.opa.enabled }}
          - name: opa-sidecar
            image: {{ .Values.opa.image }}
            imagePullPolicy: IfNotPresent
            args:
            - "run"
            - "--server"
            - "--addr=0.0.0.0:{{ .Values.opa.port }}"
            - "--config-file=/config/config.yaml"
            - "/policies/policy.rego"
            - "/opa-data/data.yaml"  # Load YAML file (OPA supports YAML natively)
            ports:
            - name: opa
              containerPort: {{ .Values.opa.port }}
              protocol: TCP
            - name: opa-grpc
              containerPort: {{ .Values.opa.grpcPort }}
              protocol: TCP
            resources:
              {{- toYaml .Values.opa.resources | nindent 14 }}
            securityContext:
              runAsUser: {{ .Values.opa.uid }}
              runAsGroup: {{ .Values.opa.gid }}
              runAsNonRoot: true
              allowPrivilegeEscalation: false
              capabilities:
                drop:
                - ALL
            volumeMounts:
            - name: opa-policy
              mountPath: /policies
              readOnly: true
            - name: opa-config
              mountPath: /config
              readOnly: true
            - name: opa-data
              mountPath: /opa-data
              readOnly: true
            livenessProbe:
              httpGet:
                path: /health
                port: {{ .Values.opa.port }}
              initialDelaySeconds: 10
              periodSeconds: 5
            readinessProbe:
              httpGet:
                path: /health
                port: {{ .Values.opa.port }}
              initialDelaySeconds: 5
              periodSeconds: 5
          {{- end }}

          {{- if .Values.xds.enabled }}
          - name: xds-service
            image: "{{ .Values.xds.image.repository }}:{{ .Values.xds.image.tag }}"
            imagePullPolicy: {{ .Values.xds.image.pullPolicy }}
            env:
            - name: SDS_GRPC_PORT
              value: "{{ .Values.xds.grpcPort }}"
            - name: SDS_HTTP_PORT
              value: "{{ .Values.xds.httpPort }}"
            - name: OPA_URL
              value: "http://localhost:{{ .Values.opa.port }}"
            - name: CA_CERT_PATH
              value: "/ca-secret/tls.crt"
            - name: CA_KEY_PATH
              value: "/ca-secret/tls.key"
            ports:
            - name: grpc
              containerPort: {{ .Values.xds.grpcPort }}
              protocol: TCP
            - name: http
              containerPort: {{ .Values.xds.httpPort }}
              protocol: TCP
            resources:
              {{- toYaml .Values.xds.resources | nindent 14 }}
            securityContext:
              runAsUser: {{ .Values.xds.uid }}
              runAsGroup: {{ .Values.xds.gid }}
              runAsNonRoot: true
              allowPrivilegeEscalation: false
              capabilities:
                drop:
                - ALL
              readOnlyRootFilesystem: false
            volumeMounts:
            - name: ca-secret
              mountPath: /ca-secret
              readOnly: false  # xDS needs write access to generate CA
            livenessProbe:
              httpGet:
                path: /health
                port: {{ .Values.xds.httpPort }}
              initialDelaySeconds: 5
              periodSeconds: 10
            readinessProbe:
              httpGet:
                path: /health
                port: {{ .Values.xds.httpPort }}
              initialDelaySeconds: 2
              periodSeconds: 5
          {{- end }}

  # Inject volumes
  - name: inject-volumes
    match:
      any:
      - resources:
          kinds:
          - Pod
          selector:
            matchLabels:
              "{{ .Values.kyverno.annotationKey }}": "{{ .Values.kyverno.annotationValue }}"
    mutate:
      patchStrategicMerge:
        spec:
          volumes:
          - name: ca-secret
            emptyDir: {}  # xDS service generates CA at runtime and writes it here
          - name: ca-certificates
            emptyDir: {}
          - name: ca-certificates-system
            hostPath:
              path: /etc/ssl/certs
              type: Directory
          - name: envoy-config
            configMap:
              name: {{ .Release.Name }}-envoy-config
          {{- if .Values.opa.enabled }}
          - name: opa-policy
            configMap:
              name: {{ .Release.Name }}-opa-policy
          - name: opa-config
            configMap:
              name: {{ .Release.Name }}-opa-config
          - name: opa-data
            emptyDir: {}  # Shared volume for init container to prepare data for OPA
          - name: podinfo
            downwardAPI:
              items:
              - path: "opa-data"
                fieldRef:
                  fieldPath: metadata.annotations['intercept-proxy/opa-data']
              defaultMode: 0444
          {{- end }}

  # Inject CA certificates into all containers (except sidecars)
  - name: inject-ca-certs
    match:
      any:
      - resources:
          kinds:
          - Pod
          selector:
            matchLabels:
              "{{ .Values.kyverno.annotationKey }}": "{{ .Values.kyverno.annotationValue }}"
    mutate:
      foreach:
      - list: "request.object.spec.containers[]"
        preconditions:
          all:
          - key: "{{ `{{ element.name }}` }}"
            operator: NotEquals
            value: envoy-proxy
          - key: "{{ `{{ element.name }}` }}"
            operator: NotEquals
            value: opa-sidecar
          - key: "{{ `{{ element.name }}` }}"
            operator: NotEquals
            value: xds-service
        patchStrategicMerge:
          spec:
            containers:
            - name: "{{ `{{ element.name }}` }}"
              env:
              - name: SSL_CERT_FILE
                value: "/etc/ssl/certs/ca-certificates.crt"
              - name: CA_BUNDLE
                value: "/etc/ssl/certs/ca-certificates.crt"
              - name: CURL_CA_BUNDLE
                value: "/etc/ssl/certs/ca-certificates.crt"
              - name: REQUESTS_CA_BUNDLE
                value: "/etc/ssl/certs/ca-certificates.crt"
              - name: NODE_EXTRA_CA_CERTS
                value: "/etc/ssl/certs/intercept-ca.crt"
              volumeMounts:
              - name: ca-certificates
                mountPath: /etc/ssl/certs
                readOnly: true

  {{- if .Values.kyverno.injectEnvVars }}
  # Inject environment variables into all containers
  - name: inject-proxy-env-vars
    match:
      any:
      - resources:
          kinds:
          - Pod
          selector:
            matchLabels:
              "{{ .Values.kyverno.annotationKey }}": "{{ .Values.kyverno.annotationValue }}"
    mutate:
      foreach:
      - list: "request.object.spec.containers[]"
        patchStrategicMerge:
          spec:
            containers:
            - name: "{{ `{{ element.name }}` }}"
              env:
              - name: HTTP_PROXY
                value: {{ .Values.kyverno.envVars.HTTP_PROXY | quote }}
              - name: HTTPS_PROXY
                value: {{ .Values.kyverno.envVars.HTTPS_PROXY | quote }}
              - name: NO_PROXY
                value: {{ .Values.kyverno.envVars.NO_PROXY | quote }}
              - name: SSL_CERT_FILE
                value: "/usr/local/share/ca-certificates/intercept-ca.crt"
              volumeMounts:
              - name: ca-certificates
                mountPath: /usr/local/share/ca-certificates
                readOnly: true
  {{- end }}